# -*- coding: utf-8 -*-
"""ANN_Lab4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LgUJ2rpL34aUVUnU4xXWb-SvBoOsf0jS
"""

import numpy as np
from tensorflow.python import keras
from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.models import Model
import matplotlib
matplotlib.use("TkAgg")
from matplotlib import pyplot as plt
import time

# from numpy.random import seed
# seed(1)
# from tensorflow.python import set_random_seed
# set_random_seed(2)

class DnnSa:
    """
    implementation of deep neural network 
    """
    def __init__(self,data):
        self.loadData(data)
        self.N = self.X_train.shape[0]
        self.D = self.X_train.shape[1]
        self.val_accs = []
        self.val_mse = []
        self.train_mse = []
        self.timeDict = {}

    def run(self,mode,Lambda = 0):
        self.mode = mode
        self.Lambda = Lambda

        self.timeLst = []
        if self.mode == "U":
            self.lst_hidden_nodes = [16,32,64,128,256,512]
        elif self.mode == "O":
            self.lst_hidden_nodes = [1024,2048,4096]
            self.lst_hidden_nodes = [1024]
        elif self.mode == "Noise":
            pass
        
        for hidden_nodes in self.lst_hidden_nodes:
            self.fit(hidden_nodes)

        self.timeDict[self.mode] = self.timeLst

    def fit(self,hidden_nodes,activation="sigmoid"):
        start = time.time()
        input_img = Input(shape=(self.D,)) # this is our input placeholder
        
        encoded = Dense(hidden_nodes, activation=activation,
                        kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                        bias_initializer=keras.initializers.Zeros(),
                        kernel_regularizer=keras.regularizers.l1(self.Lambda))(input_img) # "encoded" is the encoded representation of the input
        decoded = Dense(self.D, activation='sigmoid',
                        kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                        bias_initializer=keras.initializers.Zeros(),
                        kernel_regularizer=keras.regularizers.l1(self.Lambda))(encoded) # "decoded" is the lossy reconstruction of the input
        
        # encode
        self.encoder = Model(inputs=input_img, outputs=encoded) # encoder model: maps an input to its encoded representation
        autoencoder = Model(inputs=input_img, outputs=decoded) # this model maps an input to its reconstruction

        encoded_input = Input(shape=(hidden_nodes,)) # placeholder for encoded (32-dimensional) input
        decoder_layer = autoencoder.layers[-1] # retrieve the last layer of the autoencoder model
        self.decoder = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input)) # create the decoder model
        optimizer = keras.optimizers.SGD(lr=0.1, momentum=0.9, decay = 0, nesterov=False)
        # autoencoder.compile(optimizer=optimizer, loss="binary_crossentropy", metrics=["accuracy","mean_squared_error"])
        autoencoder.compile(optimizer=optimizer, loss="mse", metrics=["accuracy","mean_squared_error"])
        
        info = autoencoder.fit(self.X_train, self.X_train, # for training: x, y
                epochs=100,
                batch_size=50, # default: 32
                shuffle=True # shuffle each batch
                ,validation_data = (self.X_val, self.X_val) #no validation
                )
        
        # self.val_accs.append(info.history["val_acc"])
        # print(info.history["val_loss"])
        self.timeLst.append(time.time()-start)
        # self.val_mse.append(info.history["val_mean_squared_error"])
        self.train_mse.append(info.history["mean_squared_error"])

    def plotError(self,error="Training"):
        
        if error == "Training":
            errors = self.train_mse
        elif error == "Validation":
            errors = self.val_mse

        plt.figure()
        plt.xlabel('Training epochs')
        plt.ylabel('Train MSE')
        for mse, hidden_nodes, time in zip(errors,self.lst_hidden_nodes,self.timeDict[self.mode]):
            nodes = str(hidden_nodes)
            while len(nodes) < 4:
                nodes += " "
            plt.plot(mse, label="Hidden nodes:"+nodes+", Time:"+str(round(time,2))+" sec")
        plt.legend(loc='upper right')
        # plt.show()
    
    def visualize(self):
        encoded_imgs = self.encoder.predict(self.X_val)
        decoded_imgs = self.decoder.predict(encoded_imgs)

        n = 10  # how many digits we will display
        plt.figure(figsize=(20, 4))
        for i in range(n):
            # display original
            ax = plt.subplot(2, n, i + 1)
            plt.imshow(self.X_val[i].reshape(28, 28))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)

            # display reconstruction
            ax = plt.subplot(2, n, i + 1 + n)
            plt.imshow(decoded_imgs[i].reshape(28, 28))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
        # plt.show()

    def loadData(self,data):
        self.X_train = data[0]
        self.Y_train = data[1]
        self.X_val   = data[2]
        self.Y_val   = data[3]
        # print(self.X_train.shape)
        # print(self.Y_train.shape)
        # print(self.X_val.shape)
        # print(self.Y_val.shape)

        # plt.hist(self.Y_val) # pretty balanced
        # plt.show()
        # plt.hist(self.Y_train) # pretty balanced
        # plt.show()
        
        # plt.figure()
        # num = 10
        # f, ax = plt.subplots(1, num)
        # for i in range(num):
        #     img = self.X_train[i].reshape(28, 28)
        #     ax[i].imshow(img)
        # plt.show()

def readData():
    bindigit_trn    = np.genfromtxt('binMNIST_data/bindigit_trn.csv',    delimiter=',')
    targetdigit_trn = np.genfromtxt('binMNIST_data/targetdigit_trn.csv', delimiter=',')
    bindigit_tst    = np.genfromtxt('binMNIST_data/bindigit_tst.csv',    delimiter=',')
    targetdigit_tst = np.genfromtxt('binMNIST_data/targetdigit_tst.csv', delimiter=',')

    return np.array([bindigit_trn, targetdigit_trn, bindigit_tst, targetdigit_tst])

if __name__ == '__main__':
    data = readData()
    # x = np.random.randint(2,size=100)
    # plt.hist(x)
    # plt.show()
    a = time.time()
    DNN = DnnSa(data)
    # DNN.run(mode="U")
    # DNN.plotError()
    for L in [0.01,0.1,1,10]:
        DNN.run(mode="O",Lambda=L)
        DNN.plotError()
        DNN.visualize()
    print(((time.time()-a)/60)/60)
    plt.show()